\chapter{Evaluation}
\label{eval}
This chapter discusses the evaluations that were carried out on the project.  In particular, it discusses the approaches that were taken, the results that were collected and the conclusions that can be drawn from the results, including future work for both the evaluations and the product.

It was necessary to perform evaluations with the assistance of people who were not directly involved with the project because an insider might not be able to provide a completely impartial examination of the system.

\section{General Approach}
The evaluation took two forms: firstly, a basic usability evaluation, focussing on a single user's interactions with the system; and secondly, an extended evaluation, examining the positive points and shortcomings of the system when multiple users were working with the system.

It was hoped that a sample of users external to the project, with various levels of knowledge of \bibtex and \LaTeX, would participate in the study.  Sampling a range of users was intended to ensure that the evaluation collected a representative spread of opinions on the product for analysis.

As the evaluation would involve other people, the `School of Computing Science Ethics Checklist'\footnote{The signed ethics checklist document is included as an appendix to this document.} was consulted to ensure that the intended evaluation was ethically sound and that it would not put participants at any greater risk than they encounter in their normal working lives.

The evaluations had slightly different environments and execution, which will be discussed within the relevant sections below.  The purpose of the two different evaluations are explained, and results are included with each of them.

\section{Basic Usability Evaluation}
The basic usability evaluation is based around the points raised in the background survey (Chapter \ref{backgrnd}).  Recall that the examination criteria in the background survey were centred around the \gls{ui} to the system, the features of the system and how fault tolerant and robust the system was.

\subsection{Environment}
The participant sat at a desk on an adjustable-height chair in a well-lit, quiet room at a comfortable temperature.	 Room 620 in the Boyd-Orr Building was the location for some of the experiments, others took place in the participant's office and some took place at the home of the developer.  The location and atmosphere of evaluations are mentioned to draw attention to the developer's conscious decision not to hold evaluations in an environment that was any more stressful, uncomfortable, abnormal or otherwise difficult than daily working conditions.

\subsection{Execution}
The following terminology is used consistently throughout this section: the `participant' means a volunteer who participated in an evaluation of the system and the `host' means the person who was running the evaluation; in all cases, the host was the developer of the system.

Evaluations were structured as follows:
\begin{enumerate}
	\item The host presented the participant with introduction script;\footnote{Introduction and debrief scripts are included as appendices to this document}
	\item The participant read the introduction script;
	\item The host asked the participant to verbally confirm that they agreed to take part in the evaluation;
	\item The participant agreed to take part;
	\item The participant was asked to familiarise themselves with the system by using the site for as long as they wished, and was encouraged to ask questions of the host;
	\item The participant told the host that they were ready to proceed;
	\item The host cleared the system and presented the participant with the task list\footnote{The task list is included as an appendix} and observed the participant, noting any points raised while performing tasks\footnote{The \gls{url} given to participants for import pointed to a search across the \gls{acm} library: \url{http://toms.acm.org/Volumes/V37.html?searchterm=bibtex}};
	\item The participant told the host that they had completed the task list;
	\item The host gave the participant a questionnaire, which they filled in. Notes taken by the host on the participant's behalf were given to them to remind them of things they had mentioned;
	\item The participant gave the host the completed questionnaire, which was put into an opaque folder in a random order, to help preserve participants' anonymity;
	\item The host gave the participant the debrief script;
	\item The participant took a note of the email addresses provided and was given a final chance to ask questions during the evaluation;
	\item The host thanked the participant for their time.
\end{enumerate}

While the participant was performing tasks, the host noted relevant points that the participant raised.  This was done in an attempt to help the participant to focus on the task at hand, rather than leaving the participant to recall the comments they made; it also allowed the host to gain a better insight into any difficulties encountered by the participant.

\section{Extended Usability Evaluation}
The second type of usability evaluation was aimed at testing the multi-user functions of the system, in particular the effectiveness of the system to keep users up to date with what has happened. 

\subsection{Environment}
A single session was held in room 620 of the Boyd-Orr Building, under the same conditions as the basic evaluation previous to it.

\subsection{Execution}
The execution involved 3 participants:
\begin{enumerate}
	\item One used a Mac to access the system running the Google Chrome browser;
	\item A second user worked on the lab's Linux system running the Firefox browser;
	\item The third user used a Windows 7 system and opted to use the Google Chrome browser;
\end{enumerate}

The remainder of execution of the evaluation was very similar to the basic evaluation, with the exception of the following two points:
\begin{enumerate}
	\item The three participants were not individually observed, and were instead encouraged to question the host on any issues with the system or task list;
	\item The task list was given to participants, slightly staggered in time, so that an overlap of tasks would take place.
\end{enumerate}

\section{Results and Analysis}
Results were collated from the questionnaires and the findings are listed presently, numbered by the question number of the questionnaire that the result corresponds to:
\begin{enumerate}
	\item Of all of the participants who took part in the evaluations, 100\% rated their experience using \bibtex{} as low.  No participants rated their use of the package as `frequent'.  The two highest categories of participants had an equal share of the 88\% that said they had either never or rarely used \bibtex{};
	
	\item The average rating of the layout of the interface was 87\% positive.  No participant rated the layout as lower than 80\%, which is an overwhelmingly positive response.  $\frac{1}{3}$ of respondents rated the layout as very good, the highest possible rating;
	
	\item The average consistency rating of the system was 88.8\%.  No participant responded with less than an 80\% approval rating.  This is another very positive reception of the system.  44.4\% of respondents rated the consistency as very consistent, which was the highest possible rating;
	
	\item The average rating of the simplicity of the system was 93.3\%.  There were no responses at less than an 80\% approval rate of how simple the system was to use.  $\frac{2}{3}$ of respondents rated the simplicity of the system as very simple, the best possible response available;
	
	\item The intuitiveness of performing tasks on the system was rated highly, with a 78\% approval rating.  No respondent considered performing tasks on the system particularly challenging or difficult. One respondent rated the intuitiveness of performing tasks at the highest level.  
	
	\item An overview of comments is given presently \
	Positive comments regarding intuitiveness were:
	\begin{enumerate}
		\item Great layout ($\frac{1}{3}$ of respondents);
		\item Clear instructions (22.2\% of respondents);
		\item Useful labelling (22.2\% of respondents);
		\item Easy to use (11.1\% of respondents).
	\end{enumerate}
	
	Four individual negative comments were passed:
	\begin{enumerate}
		\item Placement of features (eg search);
		\item Wording of instructions;
		\item Not obvious which duplicate is to be deleted;
		\item Too much interaction to delete entry.
	\end{enumerate}
	
	Positive comments outnumbered criticisms considerably.
	
	\item The clarity of when an action had been performed was rated highly also, with a 77.8\% approval rating. Only one respondent considered clarity to be particularly low. 22.2\% of respondents rated the clarity at the maximum level
	
	\item The aesthetic appeal of the program was rated at an average of 86.6\%. The two highest groups of respondents, 44\% each, chose the maximum rating for the aesthetics of the interface; 
	
	\item The approval of the range of functions in the program was rated highly, with a mean value of 77.8\% satisfaction.  One respondent rated functionality particularly low, suggesting that it could be improved by providing the ability to click on any piece of information in the table to get all of the information related to the clicked information.  It must be highlighted again that responses were considerably positive overall.
	
	\item Participants were asked if they thought there were any missing features that they would have expected to see in a reference management system. Their responses were as follows:
	\begin{enumerate}
		\item Customisation of the order of references;
		\item Advanced search option;
		\item Ability to select subsets of uploaded data for editing;
		\item ``Remove all duplicates'' option;
		\item Projects separation for references.
	\end{enumerate}
	Four respondents declined to suggest more features for the system.
	
	\item Participants were also asked which features they were (pleasantly) surprised to see.  There responses were as follows:
	\begin{enumerate}
		\item Import entries by text
		\item Instant search results (2 respondents)
		\item The coloured highlighting when changes were made to the reference
		\item Importation of already existing \bibtex{} files
		\item Delete file
		\item Undelete file
		\item Concurrent access
		\item Quality of layout
	\end{enumerate}
	
	\item The satisfaction of the range of reference format types for import was rated at 86.6\% on average. No respondent was less than 80\% satisfied with the range of reference format types for import.
	
	\item Respondents were asked how well they thought the program coped with poorly formatted user input. On average, there was an 86.6\% satisfaction rating. Again, no respondent rated this at lower than 80\%. 33.3\% of respondents reported that the system dealt with poorly format in the best possible way.
	
	\item Respondents were asked whether they thought the software was free from visible errors and problems. 45\% believed that there were visible problems in the system, and 55\% found no issues. 
	
	\item Users were asked to rate the system overall out of ten. The mean response was 8.3 out of ten, which is considerably positive and translates to an 83\% satisfaction rating.  One user rated the system with the maximum ten out of ten.
	
	\item The final question requested additional comments on the system.  No guidance was given as to whether the comments should be positive or negative. The responses given are summarised as follows:
	
	Positive comments:
	\begin{enumerate}
		\item Simple, easy to user interface layout (4 respondents);
		\item All required functionality included;
		\item Clear instructions;
		\item Explicit buttons;
		\item Changes made explicit through use of highlights.
	\end{enumerate}
	
	Negative comments:
	\begin{enumerate}
		\item Search box should be added (to all pages);
		\item Should be able to separate to any search option in the a new sub-menu;
		\item Multi access slow when updating;
		\item Sorting of references not obvious due to large database size (for example clicking a header led to no visible change in layout);
		\item Multiple sort requests led to each action being queued, increasing waiting time;
		\item Some functions difficult to find;
		\item Downloading all entries then uploading them again fails the system.
	\end{enumerate}

\end{enumerate}
To summarise, most users appreciated the satisfying layout of the program, and others thought more features should be added, and some improved. 

\section{Potential Improvements to Evaluations}
The evaluations were felt to be quite effective, and gained a good insight into users' thoughts.  In addition to the effective approach taken, they could have been improved by:
\begin{enumerate}
	\item Using more participants overall. The greater the number of volunteers that participate in the study, the more confident the author can be in the results that are found.  It would be helpful to have more people to take part in the evaluation so that a more extensive analysis can be performed on the results, to increase validity and reliability;
	\item Using more participants who class themselves as heavy users of \bibtex{}.  It was felt that since all participants rated their use to \bibtex{} below 4, the evaluation did not target enough of the authors that it seeks to solve problems for.  A generally positive opinion has been formed of the product and it would be important in a future study to ensure that it was true for the target population;
	\item Tracking the time it took users to find items on the interface and perform tasks, which would give the evaluation temporal quantitative data for examination;
	\item A step further than (2) would be to use eyegaze techniques to analyse whether users are actually drawn to the intended navigation areas or if there is something subconsciously distracting about another area of the interface that could be altered.
\end{enumerate}

\section{Summary of Evaluations}
Results of the evaluations were found to be overwhelmingly positive.  All measures were responded to with an exceedingly positive and praising collection of ratings.  

The sample size for evaluations was quite low: only nine participants volunteered to take part.  This was a disappointing number given the effort that the author put into establishing a common time for evaluations to take place across the year group.

Some conclusions which can be drawn from the analysis of the results of these two evaluations suggest that the project was successful in its goal of providing a usable and useful system.  

It was found that the software system fell short on robustness issues in the basic evaluation. Some of the shortcomings found in evaluations were improved upon for later versions of the system.

Some encouraging suggestions which have come from the evaluations are discussed in the next and final chapter, the conclusion.  The conclusion chapter also draws this dissertation to a close with some reflection on the effectiveness of the approach taken.